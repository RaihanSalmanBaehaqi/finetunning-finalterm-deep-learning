<div align="center">

# ğŸ“ Task 3: Fine-tuning Decoder-Only LLM

### Phi-2 for Text Summarization | UAS Deep Learning

[![Phi-2](https://img.shields.io/badge/Model-Phi--2_(2.7B)-00BCF2?style=for-the-badge&logo=microsoft&logoColor=white)](https://huggingface.co/microsoft/phi-2)
[![XSum](https://img.shields.io/badge/Dataset-XSum-purple?style=for-the-badge)](https://huggingface.co/datasets/EdinburghNLP/xsum)
[![LoRA](https://img.shields.io/badge/Method-LoRA-orange?style=for-the-badge)](https://arxiv.org/abs/2106.09685)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.35+-FFD21E?style=for-the-badge)](https://huggingface.co/)

**Abstractive Summarization dengan Parameter-Efficient Fine-tuning**

[ğŸ“Š Results](#-results) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“ Structure](#-directory-structure)

---

### ğŸ¯ Performance Highlights

| ROUGE-1 | ROUGE-2 | ROUGE-L | Trainable Params | Training Time |
|:-------:|:-------:|:-------:|:----------------:|:-------------:|
| **7.13%** | **0.21%** | **6.03%** | 8.4M (0.30%) | ~1.5-2 hrs |

</div>

---

## ğŸ“‹ Overview

**Task 3** mengeksplorasi arsitektur **Decoder-Only Large Language Model (LLM)** untuk task **Abstractive Text Summarization**. Berbeda dengan Task 1 (encoder-only) dan Task 2 (encoder-decoder), task ini menggunakan **Phi-2** dari Microsoft dengan teknik **LoRA** untuk parameter-efficient fine-tuning.

### ğŸ“ Learning Objectives

| # | Objective | Status |
|:-:|:----------|:------:|
| 1 | Memahami arsitektur decoder-only (Causal LM) | âœ… |
| 2 | Implementasi LoRA untuk parameter-efficient fine-tuning | âœ… |
| 3 | Menerapkan 4-bit quantization (QLoRA) | âœ… |
| 4 | Menggunakan instruction-style prompting | âœ… |
| 5 | Evaluasi dengan ROUGE metrics | âœ… |
| 6 | Optimasi untuk consumer GPU (T4) | âœ… |

### ğŸ”„ Comparison with Other Tasks

| Aspect | Task 1 (BERT) | Task 2 (T5) | Task 3 (Phi-2) |
|:------:|:-------------:|:-----------:|:--------------:|
| **Architecture** | Encoder-only | Encoder-Decoder | **Decoder-only** |
| **Parameters** | 109M | 223M | **2.7B** |
| **Fine-tuning** | Full | Full | **LoRA (0.3%)** |
| **Task Type** | Classification | Generation | **Generation** |
| **Memory** | Standard | Standard | **Quantized (4-bit)** |

---

## ğŸ—ï¸ Architecture

### Phi-2: Decoder-Only LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DECODER-ONLY ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   Input: "### Article:\n{document}\n\n### Summary:\n"              â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚          â”‚        PHI-2 DECODER              â”‚                     â”‚
â”‚          â”‚     (32 Transformer Layers)       â”‚                     â”‚
â”‚          â”‚                                   â”‚                     â”‚
â”‚          â”‚   â€¢ Self-Attention (Causal Mask)  â”‚                     â”‚
â”‚          â”‚   â€¢ LoRA Adapters (r=16, Î±=32)    â”‚                     â”‚
â”‚          â”‚   â€¢ 4-bit Quantization            â”‚                     â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚              Output: Generated Summary (Autoregressive)            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Property | Value |
|:---------|:------|
| **Model** | `microsoft/phi-2` |
| **Total Parameters** | 2,780,428,288 (~2.78B) |
| **Trainable (LoRA)** | 8,421,376 (~8.4M) |
| **Trainable %** | **0.30%** |
| **Layers** | 32 |
| **Hidden Size** | 2560 |
| **Context Length** | 2048 tokens |

### LoRA Configuration

| Parameter | Value | Description |
|:----------|:------|:------------|
| **Rank (r)** | 16 | Low-rank dimension |
| **Alpha (Î±)** | 32 | Scaling factor |
| **Target Modules** | q_proj, k_proj, v_proj, dense | Attention layers |
| **Dropout** | 0.05 | Regularization |

---

## ğŸ“Š Results

### Training Performance

| Metric | Value |
|:------:|:-----:|
| **Initial Loss** | 2.4634 |
| **Final Loss** | 2.1901 |
| **Improvement** | **11.09%** |
| **Training Time** | ~1.5-2 hours |

### ROUGE Evaluation

| Metric | Score | Description |
|:------:|:-----:|:------------|
| **ROUGE-1** | **7.13%** | Unigram overlap |
| **ROUGE-2** | **0.21%** | Bigram overlap |
| **ROUGE-L** | **6.03%** | Longest common subsequence |

### Performance Notes

| Aspect | Status | Explanation |
|:------:|:------:|:------------|
| **Training Convergence** | âœ… Good | Loss decreased consistently |
| **ROUGE Scores** | âš ï¸ Low | Limited by 1 epoch & small data |
| **LoRA Efficiency** | âœ… Excellent | Only 0.3% params trained |
| **Memory Usage** | âœ… Efficient | Runs on T4 GPU (16GB) |

> âš ï¸ **Note:** Low ROUGE scores are expected due to training constraints (1 epoch, 0.7% of XSum data). With full training, scores would improve significantly.

---

## ğŸ“ Directory Structure

```
task-3/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              â† You are here!
â”‚
â””â”€â”€ ğŸ“‚ finetuning-phi-2-text-summarization/   â† Main project
    â”‚
    â”œâ”€â”€ ğŸ“„ README.md                          # Detailed documentation
    â”‚
    â”œâ”€â”€ ğŸ““ notebooks/
    â”‚   â””â”€â”€ finetuning-phi-2-text-summarization.ipynb  # Main notebook
    â”‚
    â””â”€â”€ ğŸ“Š reports/
        â”œâ”€â”€ ğŸ“„ report_phi2_summarization.md   # Comprehensive report
        â”œâ”€â”€ ğŸ“„ sample_predictions.txt         # Example outputs
        â”œâ”€â”€ ğŸ“„ all_predictions.csv            # All test predictions
        â”œâ”€â”€ ğŸ–¼ï¸ dataset_analysis.png           # Dataset visualization
        â”œâ”€â”€ ğŸ–¼ï¸ training_loss.png              # Loss curve
        â””â”€â”€ ğŸ–¼ï¸ Rouge_Scores.png               # ROUGE metrics
```

---

## ğŸ“š Dataset: XSum

**Extreme Summarization** - One-sentence summaries of BBC news articles.

| Split | Original | Used | Percentage |
|:-----:|:--------:|:----:|:----------:|
| Train | 204,045 | 1,500 | 0.7% |
| Test | 11,334 | 150 | 1.3% |

### Characteristics

| Property | Value |
|:---------|:------|
| **Source** | BBC News articles |
| **Summary Style** | Highly abstractive (paraphrasing) |
| **Compression** | ~18:1 ratio |
| **Challenge** | Requires rewriting, not extraction |

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|:----------|:------|
| **Epochs** | 1 |
| **Batch Size** | 1 |
| **Gradient Accumulation** | 8 |
| **Effective Batch** | 8 |
| **Learning Rate** | 2e-4 |
| **Optimizer** | paged_adamw_8bit |
| **Quantization** | 4-bit (NF4) |
| **FP16** | âœ… Enabled |
| **Gradient Checkpointing** | âœ… Enabled |

---

## ğŸš€ Quick Start

### Google Colab (Recommended)

1. **Navigate to notebook:**
   ```
   task-3/finetuning-phi-2-text-summarization/notebooks/
   ```

2. **Open in Colab:**
   - Upload `finetuning-phi-2-text-summarization.ipynb`
   - Enable GPU: `Runtime â†’ Change runtime type â†’ GPU (T4)`

3. **Run all cells:**
   ```
   Runtime â†’ Run all (Ctrl+F9)
   ```

### Local Setup

```bash
# Navigate to project
cd task-3/finetuning-phi-2-text-summarization

# Install dependencies
pip install torch transformers accelerate peft bitsandbytes
pip install trl rouge-score datasets

# Run Jupyter
jupyter notebook notebooks/finetuning-phi-2-text-summarization.ipynb
```

### Hardware Requirements

| Component | Minimum | Recommended |
|:----------|:--------|:------------|
| **GPU** | T4 (16GB) | A100 (40GB) |
| **RAM** | 12GB | 16GB+ |
| **Time** | 1.5 hrs | 1 hr |

---

## ğŸ‘¥ Team Information

| Name | NIM | Class | Task |
|:-----|:---:|:-----:|:----:|
| [Member 1] | [NIM] | TK-46-02 | Task 1 |
| [Member 2] | [NIM] | TK-46-02 | Task 2 |
| [Your Name] | [Your NIM] | TK-46-02 | **Task 3** âœ… |

---

## ğŸ“– Documentation

| Document | Description | Link |
|:---------|:------------|:----:|
| **Project README** | Detailed documentation | [ğŸ“„](finetuning-phi-2-text-summarization/README.md) |
| **Experiment Report** | Full analysis & results | [ğŸ“Š](finetuning-phi-2-text-summarization/reports/report_phi2_summarization.md) |
| **Training Notebook** | Complete implementation | [ğŸ““](finetuning-phi-2-text-summarization/notebooks/finetuning-phi-2-text-summarization.ipynb) |

---

## ğŸ”— Related Tasks

| Task | Model | Architecture | Dataset | Status |
|:----:|:-----:|:------------:|:-------:|:------:|
| [Task 1](../task-1/) | BERT | Encoder | AG News, GoEmotions, MNLI | âœ… |
| [Task 2](../task-2/) | T5 | Encoder-Decoder | SQuAD | âœ… |
| **Task 3** | **Phi-2** | **Decoder** | **XSum** | âœ… |

---

## ğŸ’¡ Key Concepts

### Why LoRA?

| Benefit | Description |
|:--------|:------------|
| **Memory Efficient** | Train only 0.3% of parameters |
| **Fast** | Much faster than full fine-tuning |
| **Portable** | Adapter weights are small (~32MB) |
| **No Forgetting** | Preserves pre-trained knowledge |

### Memory Optimization Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       MEMORY OPTIMIZATION               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ 4-bit Quantization    â†’ 75% saved   â”‚
â”‚  â€¢ LoRA Adapters         â†’ 99% saved   â”‚
â”‚  â€¢ Gradient Checkpointing â†’ 30% saved  â”‚
â”‚  â€¢ Mixed Precision (FP16) â†’ 50% saved  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Result: 2.7B model on 16GB GPU! âœ…    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“œ License

Educational project for Deep Learning course (UAS) at Telkom University.

---

<div align="center">

**Part of UAS Deep Learning**

*Exploring Decoder-Only LLM with Parameter-Efficient Fine-tuning*

[![Task](https://img.shields.io/badge/Task-3_of_3-blue?style=for-the-badge)]()
[![Status](https://img.shields.io/badge/Status-âœ…_Completed-success?style=for-the-badge)]()
[![LoRA](https://img.shields.io/badge/Trainable-0.30%25-orange?style=for-the-badge)]()

</div>
