<div align="center">

# ğŸ“ Fine-tuning Phi-2 for Text Summarization

### Task 3 - UAS Deep Learning: Decoder-Only LLM with LoRA

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.35+-FFD21E?style=for-the-badge)](https://huggingface.co/)
[![Phi-2](https://img.shields.io/badge/Microsoft-Phi--2-00BCF2?style=for-the-badge&logo=microsoft&logoColor=white)](https://huggingface.co/microsoft/phi-2)

**Abstractive Text Summarization dengan Parameter-Efficient Fine-tuning (LoRA)**

[ğŸ¯ Overview](#-overview) â€¢ [ğŸ“Š Results](#-results) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-documentation)

---

<img src="reports/training_loss.png" width="600" alt="Training Loss"/>

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Team Information](#-team-information)
- [Model Architecture](#-model-architecture)
- [Dataset](#-dataset)
- [Methodology](#-methodology)
- [Results](#-results)
- [Visualizations](#-visualizations)
- [Quick Start](#-quick-start)
- [Repository Structure](#-repository-structure)
- [Technical Details](#-technical-details)
- [Key Learnings](#-key-learnings)
- [References](#-references)

---

## ğŸ¯ Overview

Repository ini berisi implementasi **Task 3** dari UAS Deep Learning yang berfokus pada **fine-tuning Phi-2** (decoder-only LLM dengan 2.7B parameters) untuk task **Abstractive Text Summarization** menggunakan dataset **XSum**.

### Apa yang Unik dari Task Ini?

| Aspect | Task 1 & 2 | Task 3 (Phi-2) |
|:------:|:----------:|:--------------:|
| **Architecture** | Encoder / Encoder-Decoder | **Decoder-only (LLM)** |
| **Model Size** | 109M - 223M | **2.7B parameters** |
| **Fine-tuning** | Full fine-tuning | **LoRA (0.3% params)** |
| **Quantization** | None | **4-bit (QLoRA)** |
| **Memory** | Standard | **Optimized for consumer GPU** |

### ğŸ¯ Learning Objectives

1. âœ… **Fine-tune decoder-only LLM** (Phi-2) untuk text generation
2. âœ… **Implement LoRA** untuk parameter-efficient fine-tuning
3. âœ… **Apply 4-bit quantization** untuk memory efficiency
4. âœ… **Use instruction-style prompting** untuk summarization
5. âœ… **Evaluate** dengan ROUGE metrics
6. âœ… **Optimize** untuk Google Colab Free Tier (T4 GPU)

---

## ğŸ‘¥ Team Information

<table>
<tr>
<td><b>ğŸ“š Course</b></td>
<td>Deep Learning - Final Term Project</td>
</tr>
<tr>
<td><b>ğŸ“ Task</b></td>
<td>Task 3 - Fine-tuning Decoder-Only LLM for Summarization</td>
</tr>
</table>

### Group Members

| Name | NIM | Class |
|:-----|:---:|:-----:|
| Raihan Salman Baehaqi | 1103220180 | TK-46-02 |
| Jaka Kelana Wijaya | 1103223048 | TK-46-02 |

---

## ğŸ—ï¸ Model Architecture

### Phi-2: Decoder-Only LLM

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DECODER-ONLY ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Input: "### Article:\n{document}\n\n### Summary:\n"           â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚          â”‚        PHI-2 DECODER              â”‚                  â”‚
â”‚          â”‚       (32 Transformer Layers)     â”‚                  â”‚
â”‚          â”‚                                   â”‚                  â”‚
â”‚          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                  â”‚
â”‚          â”‚   â”‚   Self-Attention        â”‚    â”‚  â† Causal Mask   â”‚
â”‚          â”‚   â”‚   (with LoRA adapters)  â”‚    â”‚    (å·¦â†’å³ only)  â”‚
â”‚          â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                  â”‚
â”‚          â”‚              â”‚                    â”‚                  â”‚
â”‚          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                  â”‚
â”‚          â”‚   â”‚   Feed-Forward Network  â”‚    â”‚                  â”‚
â”‚          â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                  â”‚
â”‚          â”‚              â”‚                    â”‚                  â”‚
â”‚          â”‚           Ã— 32 layers             â”‚                  â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚              Output: "Summary text..." (Autoregressive)         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Model Specifications

| Property | Value |
|:---------|:------|
| **Model** | `microsoft/phi-2` |
| **Developer** | Microsoft Research |
| **Type** | Decoder-only Transformer (Causal LM) |
| **Total Parameters** | **2,780,428,288** (~2.78B) |
| **Trainable Parameters** | **8,421,376** (~8.4M) with LoRA |
| **Trainable %** | **0.30%** |
| **Hidden Size** | 2560 |
| **Layers** | 32 |
| **Attention Heads** | 32 |
| **Context Length** | 2048 tokens |
| **Vocabulary** | 51,200 tokens |

### LoRA Configuration

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LoRA MECHANISM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Original Weight Matrix W (2560 Ã— 2560)                    â”‚
â”‚                     â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚         â”‚                     â”‚                              â”‚
â”‚         â–¼                     â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚    W     â”‚    +    â”‚   Î”W = BA    â”‚                     â”‚
â”‚   â”‚ (frozen) â”‚         â”‚  (trainable) â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                              â”‚                               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚                 â”‚                       â”‚
â”‚                    â–¼                 â–¼                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚ A (rÃ—d) â”‚  â†’   â”‚ B (dÃ—r) â”‚                   â”‚
â”‚              â”‚  Down   â”‚      â”‚   Up    â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚   r = 16 (low rank) â†’ 8.4M params instead of 2.78B         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

| Parameter | Value | Description |
|:----------|:------|:------------|
| **Rank (r)** | 16 | Dimension of low-rank decomposition |
| **Alpha (Î±)** | 32 | Scaling factor |
| **Scaling** | Î±/r = 2 | Applied to LoRA output |
| **Target Modules** | q_proj, k_proj, v_proj, dense | Attention layers |
| **Dropout** | 0.05 | Regularization |
| **Bias** | none | No bias training |

### Why LoRA + Quantization?

| Technique | Benefit | Memory Saving |
|:----------|:--------|:-------------:|
| **LoRA** | Train only 0.3% parameters | ~99% |
| **4-bit Quantization** | Compress weights from FP16 to INT4 | ~75% |
| **Gradient Checkpointing** | Trade compute for memory | ~30% |
| **Combined** | Run 2.7B model on T4 GPU (16GB) | âœ… |

---

## ğŸ“Š Dataset

### XSum (Extreme Summarization)

<div align="center">

<img src="reports/dataset_analysis.png" width="700" alt="Dataset Analysis"/>

</div>

### Dataset Statistics

| Split | Original | Used | Percentage |
|:-----:|:--------:|:----:|:----------:|
| **Train** | 204,045 | **1,500** | 0.7% |
| **Test** | 11,334 | **150** | 1.3% |
| **Total** | 226,711 | **1,650** | ~0.7% |

### Why Small Subset?

| Constraint | Reason |
|:-----------|:-------|
| **GPU Memory** | Phi-2 (2.7B) requires significant VRAM |
| **Training Time** | Full dataset would take days |
| **Colab Limits** | Free tier has session timeouts |
| **Educational** | Sufficient to demonstrate pipeline |

### Dataset Characteristics

| Property | Value |
|:---------|:------|
| **Source** | BBC News articles |
| **Summary Style** | One-sentence, highly abstractive |
| **Avg Document** | ~430 words |
| **Avg Summary** | ~23 words |
| **Compression** | ~18:1 ratio |
| **Challenge** | Requires paraphrasing, not extraction |

### Input Format (Instruction-style)

```python
prompt_template = """### Article:
{document}

### Summary:
{summary}"""
```

---

## ğŸ”§ Methodology

### Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PHI-2 SUMMARIZATION PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  1. ğŸ“¥ Load XSum Dataset (1,500 train / 150 test)                â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  2. ğŸ”„ Format with Instruction Prompt Template                   â”‚
â”‚         â”‚   "### Article:\n{doc}\n\n### Summary:\n{sum}"         â”‚
â”‚         â–¼                                                         â”‚
â”‚  3. ğŸ¤– Load Phi-2 with 4-bit Quantization (QLoRA)                â”‚
â”‚         â”‚   BitsAndBytesConfig(load_in_4bit=True)                â”‚
â”‚         â–¼                                                         â”‚
â”‚  4. âš™ï¸  Apply LoRA Adapters (r=16, Î±=32)                          â”‚
â”‚         â”‚   Only 8.4M / 2.78B parameters trainable               â”‚
â”‚         â–¼                                                         â”‚
â”‚  5. ğŸ”¤ Tokenize with Phi-2 Tokenizer                             â”‚
â”‚         â”‚   max_length=512                                        â”‚
â”‚         â–¼                                                         â”‚
â”‚  6. ğŸ‹ï¸ Train with SFTTrainer (1 epoch)                           â”‚
â”‚         â”‚   Supervised Fine-Tuning on instruction data           â”‚
â”‚         â–¼                                                         â”‚
â”‚  7. ğŸ“Š Generate Summaries & Compute ROUGE                        â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  8. ğŸ’¾ Save LoRA Adapters & Generate Report                      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Configuration

| Parameter | Value | Rationale |
|:----------|:------|:----------|
| **Epochs** | 1 | Time constraint (Colab) |
| **Batch Size** | 1 | GPU memory limit |
| **Gradient Accumulation** | 8 | Effective batch = 8 |
| **Learning Rate** | 2e-4 | Standard for LoRA |
| **Optimizer** | paged_adamw_8bit | Memory efficient |
| **LR Scheduler** | Cosine | Smooth decay |
| **Warmup Steps** | 30 | Prevent early divergence |
| **Max Sequence** | 512 tokens | Balance quality vs memory |
| **FP16** | âœ… Enabled | Mixed precision training |
| **Gradient Checkpointing** | âœ… Enabled | Memory optimization |

---

## ğŸ“ˆ Results

### Training Performance

<div align="center">

<img src="reports/training_loss.png" width="600" alt="Training Loss"/>

</div>

| Metric | Value |
|:------:|:-----:|
| **Initial Loss** | 2.4634 |
| **Final Loss** | 2.1901 |
| **Improvement** | **11.09%** |
| **Total Steps** | 180 |
| **Training Time** | ~1.5-2 hours |

### ROUGE Evaluation Metrics

<div align="center">

<img src="reports/Rouge_Scores.png" width="600" alt="ROUGE Scores"/>

</div>

| Metric | Score | Description |
|:------:|:-----:|:------------|
| **ROUGE-1** | **7.13%** | Unigram overlap |
| **ROUGE-2** | **0.21%** | Bigram overlap |
| **ROUGE-L** | **6.03%** | Longest common subsequence |

### Performance Analysis

| Aspect | Status | Notes |
|:------:|:------:|:------|
| **Training Convergence** | âœ… Good | Loss decreased consistently |
| **ROUGE Scores** | âš ï¸ Low | Expected with 1 epoch, small data |
| **LoRA Efficiency** | âœ… Excellent | Only 0.3% params trained |
| **Memory Usage** | âœ… Efficient | Runs on T4 GPU (16GB) |

### Why Low ROUGE Scores?

| Factor | Impact | Solution |
|:-------|:------:|:---------|
| **Only 1 Epoch** | High | Train for 3-5 epochs |
| **Tiny Dataset** (1.5K) | High | Use full XSum (204K) |
| **XSum Difficulty** | Medium | Highly abstractive summaries |
| **No Beam Search** | Low | Use beam_size > 1 |

### Benchmark Comparison

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | Data |
|:------|:-------:|:-------:|:-------:|:----:|
| PEGASUS | 47.21 | 24.56 | 39.25 | 100% |
| BART-large | 45.14 | 22.27 | 37.25 | 100% |
| T5-large | 43.52 | 21.55 | 36.69 | 100% |
| **Phi-2 (Ours)** | **7.13** | **0.21** | **6.03** | **0.7%** |

> âš ï¸ **Note:** Our scores are significantly lower due to using only 0.7% of training data and 1 epoch. This demonstrates the trade-off between resources and performance.

---

## ğŸ–¼ï¸ Visualizations

### Training Visualization

<div align="center">

| Training Loss | ROUGE Scores |
|:-------------:|:------------:|
| <img src="reports/training_loss.png" width="400"/> | <img src="reports/Rouge_Scores.png" width="400"/> |

</div>

### Dataset Analysis

<div align="center">

<img src="reports/dataset_analysis.png" width="700" alt="Dataset Analysis"/>

</div>

---

## ğŸš€ Quick Start

### Option 1: Google Colab (Recommended) â­

1. **Upload notebook ke Google Colab**

2. **Enable GPU Runtime:**
   ```
   Runtime â†’ Change runtime type â†’ GPU (T4)
   ```

3. **Install dependencies (Cell 1):**
   ```python
   !pip install transformers accelerate peft bitsandbytes trl
   !pip install rouge-score sentencepiece datasets
   ```

4. **Run all cells:**
   ```
   Runtime â†’ Run all (Ctrl+F9)
   ```

### Option 2: Local Setup

```bash
# 1. Clone repository
git clone https://github.com/[username]/finetuning-phi-2-text-summarization.git
cd finetuning-phi-2-text-summarization

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. Install dependencies
pip install torch transformers accelerate peft bitsandbytes
pip install trl rouge-score sentencepiece datasets
pip install matplotlib seaborn pandas jupyter

# 4. Run Jupyter
jupyter notebook notebooks/finetuning-phi-2-text-summarization.ipynb
```

### Hardware Requirements

| Component | Minimum | Recommended |
|:----------|:--------|:------------|
| **GPU** | T4 (16GB) | A100 (40GB) |
| **RAM** | 12GB | 16GB+ |
| **Storage** | 10GB | 20GB |
| **Platform** | Google Colab | Local/Cloud |

---

## ğŸ“ Repository Structure

```
finetuning-phi-2-text-summarization/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          â† You are here!
â”‚
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â””â”€â”€ finetuning-phi-2-text-summarization.ipynb  # Main notebook
â”‚
â””â”€â”€ ğŸ“Š reports/
    â”œâ”€â”€ ğŸ“„ report_phi2_summarization.md   # Detailed report
    â”œâ”€â”€ ğŸ“„ sample_predictions.txt         # Example outputs
    â”œâ”€â”€ ğŸ“„ all_predictions.csv            # All test predictions
    â”œâ”€â”€ ğŸ–¼ï¸ dataset_analysis.png           # Dataset visualization
    â”œâ”€â”€ ğŸ–¼ï¸ training_loss.png              # Loss curve
    â”œâ”€â”€ ğŸ–¼ï¸ Rouge_Scores.png               # ROUGE metrics
    â””â”€â”€ ğŸ–¼ï¸ Screenshot 2026-01-12...png    # Additional visualization
```

---

## ğŸ”¬ Technical Details

### 4-bit Quantization Configuration

```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,              # Enable 4-bit loading
    bnb_4bit_quant_type="nf4",      # NormalFloat4 quantization
    bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16
    bnb_4bit_use_double_quant=True, # Double quantization
)
```

### LoRA Configuration

```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,                           # Low-rank dimension
    lora_alpha=32,                  # Scaling factor
    target_modules=[                # Modules to adapt
        "q_proj", "k_proj", 
        "v_proj", "dense"
    ],
    lora_dropout=0.05,              # Dropout for regularization
    bias="none",                    # No bias training
    task_type="CAUSAL_LM"           # Causal language modeling
)
```

### Prompt Template

```python
def format_prompt(document, summary=None):
    prompt = f"""### Article:
{document}

### Summary:
"""
    if summary:
        prompt += summary
    return prompt
```

### Generation Function

```python
def generate_summary(document, model, tokenizer, max_length=100):
    prompt = format_prompt(document)
    
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            num_beams=4,
            do_sample=False,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Extract only generated part
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary = generated.split("### Summary:\n")[-1].strip()
    
    return summary
```

### ROUGE Computation

```python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(
    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],
    use_stemmer=True
)

def compute_rouge(predictions, references):
    scores = {k: [] for k in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']}
    
    for pred, ref in zip(predictions, references):
        result = scorer.score(ref, pred)
        for key in scores:
            scores[key].append(result[key].fmeasure)
    
    return {k: np.mean(v) for k, v in scores.items()}
```

---

## ğŸ’¡ Key Learnings

### 1. LoRA: Parameter-Efficient Fine-tuning

```python
# Full fine-tuning: 2.78B parameters
# LoRA fine-tuning: 8.4M parameters (0.3%)

# Savings:
trainable_ratio = 8.4M / 2780M = 0.30%
memory_savings = ~99%
```

### 2. Instruction-style Prompting

```python
# Generic prompt (bad)
"Summarize: {document}"

# Instruction prompt (better)
"""### Article:
{document}

### Summary:
"""
# â†’ Clearer structure for the model to follow
```

### 3. Decoder-only vs Encoder-Decoder

| Aspect | Encoder-Decoder (T5) | Decoder-only (Phi-2) |
|:-------|:---------------------|:---------------------|
| **Input Processing** | Bidirectional | Left-to-right only |
| **Output** | Separate decoder | Same model generates |
| **Training** | Seq2Seq loss | Causal LM loss |
| **Memory** | Two components | Single component |
| **Best For** | Structured tasks | Open-ended generation |

### 4. Memory Optimization Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MEMORY OPTIMIZATION             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. 4-bit Quantization     â†’ 75% saved  â”‚
â”‚  2. LoRA Adapters          â†’ 99% saved  â”‚
â”‚  3. Gradient Checkpointing â†’ 30% saved  â”‚
â”‚  4. Mixed Precision (FP16) â†’ 50% saved  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Result: 2.7B model runs on 16GB GPU!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”® Potential Improvements

| Improvement | Expected Impact | Difficulty |
|:------------|:---------------:|:----------:|
| **More epochs (3-5)** | +10-15% ROUGE | â­ Easy |
| **Full XSum dataset** | +20-30% ROUGE | â­â­ Medium |
| **Larger LoRA rank (r=32)** | +3-5% ROUGE | â­ Easy |
| **Beam search tuning** | +2-3% ROUGE | â­ Easy |
| **Better prompt engineering** | +5-10% ROUGE | â­ Easy |
| **Use Phi-3 or Llama** | +5-10% ROUGE | â­â­ Medium |

---

## ğŸ“š References

1. Microsoft Research. (2023). **"Phi-2: The surprising power of small language models"**
2. Hu, E., et al. (2021). **"LoRA: Low-Rank Adaptation of Large Language Models"**
3. Narayan, S., et al. (2018). **"Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"** (XSum)
4. HuggingFace PEFT Documentation
5. HuggingFace TRL (Transformer Reinforcement Learning) Library

---

## ğŸ“œ License

This project is created for **educational purposes** as part of Deep Learning course final exam (UAS) at Telkom University.

---

<div align="center">

### â­ Star this repository if you found it helpful!

**Part of UAS Deep Learning - Task 3**

*Fine-tuning Decoder-Only LLM with Parameter-Efficient Methods*

![Status](https://img.shields.io/badge/Status-âœ…_Completed-success?style=for-the-badge)
![Model](https://img.shields.io/badge/Model-Phi--2_(2.7B)-blue?style=for-the-badge)
![Method](https://img.shields.io/badge/Method-LoRA_(0.3%25)-orange?style=for-the-badge)

</div>
