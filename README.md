# ğŸ§  Fine-tuning Transformers for NLP Tasks

### Ujian Akhir Semester (UAS) - Deep Learning

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Transformers-FFD21E?style=for-the-badge)](https://huggingface.co)
[![Colab](https://img.shields.io/badge/Google%20Colab-Ready-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com)

**Eksplorasi Tiga Arsitektur Transformer untuk Natural Language Understanding & Generation**

[ğŸ“° Task 1: BERT](#user-content--task-1-bert-text-classification--nli) â€¢
[â“ Task 2: T5](#user-content--task-2-t5-question-answering) â€¢
[ğŸ“ Task 3: Phi-2](#user-content--task-3-phi-2-text-summarization) â€¢
[ğŸš€ Quick Start](#user-content--quick-start)

---

## ğŸ‘¤ Identitas Mahasiswa

* RAIHAN SALMAN BAEHAQI (1103220180)
* JAKA KELANA WIJAYA (1103223048)

---

## ğŸ“‹ Deskripsi Proyek

Repository ini berisi implementasi **komprehensif** untuk Ujian Akhir Semester mata kuliah **Deep Learning** yang mengeksplorasi **tiga arsitektur Transformer berbeda** untuk menyelesaikan berbagai task NLP:

| ğŸ—ï¸ Architecture | ğŸ¤– Model | ğŸ“Š Task | ğŸ¯ Best Result |
|:---------------:|:-------:|:------:|:-------------:|
| **Encoder** | BERT-base | Text Classification & NLI | **94.75%** Accuracy |
| **Encoder-Decoder** | T5-base | Question Answering | **77.59%** F1 Score |
| **Decoder** | Phi-2 | Text Summarization | **7.13%** ROUGE-1 |


```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMER ARCHITECTURES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   ENCODER-ONLY          ENCODER-DECODER         DECODER-ONLY            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ ENCODER â”‚          â”‚ENCODERâ”‚ DECODER â”‚      â”‚ DECODER â”‚             â”‚
â”‚   â”‚ (BERT)  â”‚          â”‚  (T5) â”‚  (T5)   â”‚      â”‚ (Phi-2) â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚
â”‚        â”‚                   â”‚        â”‚                â”‚                   â”‚
â”‚        â–¼                   â–¼        â–¼                â–¼                   â”‚
â”‚   Classification      Seq2Seq Generation      Text Generation           â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## ğŸ† Highlights & Pencapaian


| ğŸ“° AG News | ğŸ˜Š GoEmotions | ğŸ”— MNLI | â“ SQuAD | ğŸ“ XSum |
|:----------:|:------------:|:-------:|:--------:|:-------:|
| **94.75%** | **57.49%** | **84.67%** | **77.59%** | **7.13%** |
| Accuracy | Micro-F1 | Accuracy | F1 Score | ROUGE-1 |
| âœ… Exceeds Benchmark | âœ… Matches Benchmark | âœ… Matches Benchmark | âœ… Good | âš ï¸ Limited |


### ğŸ¯ Key Achievements

- âœ… **AG News:** Melampaui benchmark BERT paper (94.75% vs 94.2%)
- âœ… **GoEmotions:** Mendekati benchmark paper (57.49% vs 58.0%)
- âœ… **MNLI Matched:** Menyamai benchmark BERT paper (84.67% vs 84.6%)
- âœ… **MNLI Mismatched:** Melampaui benchmark (84.74% vs 83.4%)
- âœ… **SQuAD QA:** Performa solid dengan 60% Exact Match & 77.59% F1
- âš ï¸ **XSum:** Performa terbatas karena constraint training (1 epoch, small dataset)

---

## ğŸ“ Struktur Repository

```
finetunning-finalterm-deep-learning/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              â† You are here!
â”‚
â”œâ”€â”€ ğŸ“‚ task-1/                                â† BERT: Encoder Architecture
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ finetuning-bert-text-classification/
â”‚   â”‚   â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ finetune_bert_ag_news.ipynb        # ğŸ“° News Classification
â”‚   â”‚   â”‚   â””â”€â”€ finetune_bert_go_emotions.ipynb    # ğŸ˜Š Emotion Detection
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š reports/
â”‚   â”‚   â”‚   â”œâ”€â”€ reports-ag-news/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ report_ag_news.md
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.png
â”‚   â”‚   â”‚   â””â”€â”€ reports-go-emotions/
â”‚   â”‚   â”‚       â”œâ”€â”€ report_go_emotions.md
â”‚   â”‚   â”‚       â””â”€â”€ *.png
â”‚   â”‚   â””â”€â”€ ğŸ“„ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ finetuning-bert-nli/
â”‚       â”œâ”€â”€ ğŸ““ notebooks/
â”‚       â”‚   â””â”€â”€ finetune_bert_mnli.ipynb           # ğŸ”— Natural Language Inference
â”‚       â”œâ”€â”€ ğŸ“Š reports/
â”‚       â”‚   â”œâ”€â”€ report_mnli.md
â”‚       â”‚   â””â”€â”€ *.png
â”‚       â””â”€â”€ ğŸ“„ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“‚ task-2/                                â† T5: Encoder-Decoder Architecture
â”‚   â””â”€â”€ ğŸ“‚ finetuning-t5-question-answering/
â”‚       â”œâ”€â”€ ğŸ““ notebooks/
â”‚       â”‚   â””â”€â”€ finetuning_t5_question_answering.ipynb  # â“ Question Answering
â”‚       â”œâ”€â”€ ğŸ“Š reports/
â”‚       â”‚   â”œâ”€â”€ report_t5_qa.md
â”‚       â”‚   â””â”€â”€ *.png
â”‚       â””â”€â”€ ğŸ“„ requirements.txt
â”‚
â””â”€â”€ ğŸ“‚ task-3/                                â† Phi-2: Decoder Architecture
    â””â”€â”€ ğŸ“‚ finetuning-phi-2-text-summarization/
        â”œâ”€â”€ ğŸ““ notebooks/
        â”‚   â””â”€â”€ finetuning-phi-2-text-summarization.ipynb  # ğŸ“ Summarization
        â”œâ”€â”€ ğŸ“Š reports/
        â”‚   â”œâ”€â”€ report_phi2_summarization.md
        â”‚   â””â”€â”€ dataset_analysis.png
        â””â”€â”€ ğŸ“„ requirements.txt
```

---

## ğŸ”µ Task 1: BERT (Text Classification & NLI)


### Architecture: **Encoder-Only (Bidirectional)**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      BERT Encoder           â”‚
     Input â”€â”€â”€â”€â”€â”€â–º  â”‚  [CLS] Token Token [SEP]   â”‚ â”€â”€â”€â”€â”€â”€â–º Classification
                    â”‚      Bidirectional          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


**BERT** (Bidirectional Encoder Representations from Transformers) memproses teks secara **bidirectional**, memungkinkan pemahaman konteks yang lebih baik untuk task klasifikasi dan pemahaman bahasa.

### ğŸ“Š Results Overview

| Task | Dataset | Type | Classes | Metric | Result | Status |
|:----:|:-------:|:----:|:-------:|:------:|:------:|:------:|
| ğŸ“° | AG News | Multi-class | 4 | Accuracy | **94.75%** | âœ… Exceeds |
| ğŸ˜Š | GoEmotions | Multi-label | 28 | Micro-F1 | **57.49%** | âœ… Matches |
| ğŸ”— | MNLI | 3-class NLI | 3 | Accuracy | **84.67%** | âœ… Matches |

### ğŸ“° Task 1A: AG News Classification

Klasifikasi artikel berita ke **4 kategori**: World, Sports, Business, Sci/Tech

```python
# Example
Input:  "Apple unveils new MacBook Pro with M3 chip at special event"
Output: "Sci/Tech" âœ…
```

<details>
<summary><b>ğŸ“ˆ Per-Class Performance</b></summary>

| Class | Precision | Recall | F1-Score |
|:-----:|:---------:|:------:|:--------:|
| ğŸŒ World | 96.79% | 95.11% | 95.94% |
| âš½ Sports | 98.64% | 99.11% | **98.87%** |
| ğŸ’¼ Business | 91.09% | 92.58% | 91.83% |
| ğŸ”¬ Sci/Tech | 92.55% | 92.21% | 92.38% |

</details>

### ğŸ˜Š Task 1B: GoEmotions Detection

Deteksi **multiple emosi** dalam teks Reddit (28 kategori emosi)

```python
# Example
Input:  "Thank you so much! This made my day!"
Output: ["gratitude", "joy", "admiration"] âœ…
```

<details>
<summary><b>ğŸ“ˆ Performance Tiers</b></summary>

| Tier | F1 Range | Emotions |
|:----:|:--------:|:---------|
| ğŸŸ¢ Excellent | 70-92% | gratitude, amusement, love, admiration |
| ğŸŸ¡ Good | 50-70% | neutral, fear, joy, remorse, optimism |
| ğŸŸ  Moderate | 30-50% | anger, curiosity, desire, disgust |
| ğŸ”´ Poor | 0-30% | annoyance, disappointment, realization |
| âš« Zero | 0% | grief, pride, relief, nervousness, embarrassment |

</details>

### ğŸ”— Task 1C: MNLI (Natural Language Inference)

Menentukan hubungan logika antara **premise** dan **hypothesis**

```python
# Example
Premise:    "A man is playing guitar on stage"
Hypothesis: "Someone is performing music"
Output:     "Entailment" âœ…
```

<details>
<summary><b>ğŸ“ˆ Per-Class Performance</b></summary>

| Class | Precision | Recall | F1-Score |
|:-----:|:---------:|:------:|:--------:|
| âœ“ Entailment | 89.69% | 84.28% | 86.90% |
| â—‹ Neutral | 78.36% | 83.03% | 80.63% |
| âœ— Contradiction | 86.04% | 86.68% | 86.36% |

</details>

ğŸ“‚ **Navigate to:** [`task-1/`](task-1/)

---

## ğŸŸ¢ Task 2: T5 (Question Answering)


### Architecture: **Encoder-Decoder (Seq2Seq)**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            T5 Model                  â”‚
     Input â”€â”€â”€â”€â”€â”€â–º  â”‚   ENCODER    â”€â”€â–º    DECODER         â”‚ â”€â”€â”€â”€â”€â”€â–º Answer
  (Q + Context)     â”‚  (Understand)      (Generate)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


**T5** (Text-to-Text Transfer Transformer) menggunakan framework **text-to-text** yang unified, mengubah semua task menjadi format generasi teks.

### ğŸ“Š Results

| Dataset | Task | Exact Match | F1 Score | Training Time |
|:-------:|:----:|:-----------:|:--------:|:-------------:|
| SQuAD | Question Answering | **60.00%** | **77.59%** | ~20 min |

### ğŸ’¬ Example

```python
Context:  "Paris is the capital and largest city of France. 
          It has a population of over 2 million people."
          
Question: "What is the capital of France?"

Generated Answer: "Paris" âœ…
```

### ğŸ“ˆ Performance Breakdown

| Category | Count | Percentage |
|:--------:|:-----:|:----------:|
| âœ… Perfect Match | 14/20 | 70% |
| ğŸŸ¢ Good Match (F1â‰¥0.7) | 2/20 | 10% |
| ğŸŸ¡ Partial Match | 1/20 | 5% |
| âŒ Poor Match | 3/20 | 15% |

ğŸ“‚ **Navigate to:** [`task-2/finetuning-t5-question-answering/`](task-2/finetuning-t5-question-answering/)

---

## ğŸŸ  Task 3: Phi-2 (Text Summarization)


### Architecture: **Decoder-Only (Causal LM)**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       Phi-2 Decoder          â”‚
     Input â”€â”€â”€â”€â”€â”€â–º  â”‚   Autoregressive Generation â”‚ â”€â”€â”€â”€â”€â”€â–º Summary
   (Document)       â”‚        (LoRA Fine-tuned)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


**Phi-2** adalah model decoder-only dari Microsoft (2.7B parameters) yang di-finetune menggunakan **LoRA** untuk efisiensi.

### ğŸ“Š Results

| Dataset | Metric | Score | Training | Trainable Params |
|:-------:|:------:|:-----:|:--------:|:----------------:|
| XSum | ROUGE-1 | **7.13%** | 1 epoch | 8.4M (0.30%) |
| XSum | ROUGE-2 | **0.21%** | ~1.5 hrs | LoRA r=16 |
| XSum | ROUGE-L | **6.03%** | 4-bit quantized | Î±=32 |

### âš ï¸ Catatan Performa

Skor ROUGE rendah disebabkan oleh:
- **Training terbatas:** Hanya 1 epoch
- **Dataset kecil:** 1.5K samples (vs 204K full XSum)
- **Time constraint:** Limited computational resources

### ğŸ’¡ LoRA Efficiency

```python
# Full Fine-tuning vs LoRA
Total Parameters:     2,780,428,288 (2.7B)
Trainable (LoRA):         8,421,376 (8.4M)
Efficiency:                   0.30%  âœ…
```

ğŸ“‚ **Navigate to:** [`task-3/finetuning-phi-2-text-summarization/`](task-3/finetuning-phi-2-text-summarization/)

---

## ğŸ“Š Perbandingan Arsitektur


| Aspect | ğŸ”µ BERT (Encoder) | ğŸŸ¢ T5 (Enc-Dec) | ğŸŸ  Phi-2 (Decoder) |
|:------:|:-----------------:|:---------------:|:------------------:|
| **Direction** | Bidirectional | Seq2Seq | Autoregressive |
| **Best For** | Understanding | Translation, QA | Generation |
| **Parameters** | 109M | 223M | 2.7B |
| **Output** | Classification | Sequence | Sequence |
| **Pre-training** | MLM + NSP | Span Corruption | Next Token |


### ğŸ¯ Kapan Menggunakan Arsitektur Tertentu?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE SELECTION GUIDE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Task Type                          Recommended Architecture     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Text Classification                â†’ ENCODER (BERT)             â”‚
â”‚  Named Entity Recognition           â†’ ENCODER (BERT)             â”‚
â”‚  Sentiment Analysis                 â†’ ENCODER (BERT)             â”‚
â”‚                                                                  â”‚
â”‚  Machine Translation                â†’ ENCODER-DECODER (T5)       â”‚
â”‚  Question Answering                 â†’ ENCODER-DECODER (T5)       â”‚
â”‚  Summarization (structured)         â†’ ENCODER-DECODER (T5)       â”‚
â”‚                                                                  â”‚
â”‚  Text Generation                    â†’ DECODER (GPT, Phi-2)       â”‚
â”‚  Chatbot/Dialogue                   â†’ DECODER (GPT, Phi-2)       â”‚
â”‚  Creative Writing                   â†’ DECODER (GPT, Phi-2)       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- GPU dengan VRAM 16GB+ (atau Google Colab)
- HuggingFace account (optional, untuk model hosting)

### Option 1: Google Colab (Recommended) â­

1. **Clone repository ini**
2. **Upload notebook ke Google Colab**
3. **Enable GPU Runtime:**
   ```
   Runtime â†’ Change runtime type â†’ GPU (T4)
   ```
4. **Run all cells:**
   ```
   Runtime â†’ Run all (Ctrl+F9)
   ```

### Option 2: Local Setup

```bash
# 1. Clone repository
git clone https://github.com/[username]/finetunning-finalterm-deep-learning.git
cd finetunning-finalterm-deep-learning

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. Install dependencies (pilih task)
pip install -r task-1/finetuning-bert-text-classification/requirements.txt
# atau
pip install -r task-2/finetuning-t5-question-answering/requirements.txt
# atau
pip install -r task-3/finetuning-phi-2-text-summarization/requirements.txt

# 4. Run Jupyter
jupyter notebook
```

### â±ï¸ Estimated Training Time

| Task | Notebook | Time (T4 GPU) |
|:----:|:--------:|:-------------:|
| 1A | AG News | ~35 min |
| 1B | GoEmotions | ~10 min |
| 1C | MNLI | ~113 min |
| 2 | T5 QA | ~20 min |
| 3 | Phi-2 | ~90 min |
| **Total** | - | **~4.5 hours** |

---

## ğŸ› ï¸ Tech Stack


| Category | Technologies |
|:--------:|:-------------|
| **Framework** | ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white) |
| **Models** | ![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-FFD21E?style=flat) ![PEFT](https://img.shields.io/badge/PEFT-LoRA-blue?style=flat) |
| **Data** | ![Datasets](https://img.shields.io/badge/ğŸ¤—_Datasets-FFD21E?style=flat) |
| **Metrics** | ![scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white) |
| **Visualization** | ![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=flat) ![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?style=flat) |
| **Environment** | ![Colab](https://img.shields.io/badge/Google_Colab-F9AB00?style=flat&logo=googlecolab&logoColor=white) |


---

## ğŸ“š Reports & Documentation

| Task | Report | Visualizations |
|:----:|:------:|:--------------:|
| 1A | [ğŸ“„ AG News Report](task-1/finetuning-bert-text-classification/reports/reports-ag-news/report_ag_news.md) | Confusion Matrix, Distribution |
| 1B | [ğŸ“„ GoEmotions Report](task-1/finetuning-bert-text-classification/reports/reports-go-emotions/report_go_emotions.md) | Per-class F1, Distribution |
| 1C | [ğŸ“„ MNLI Report](task-1/finetuning-bert-nli/reports/report_mnli.md) | Confusion Matrix, Distribution |
| 2 | [ğŸ“„ T5 QA Report](task-2/finetuning-t5-question-answering/reports/report_t5_qa.md) | Loss Curves, Metrics, Examples |
| 3 | [ğŸ“„ Phi-2 Report](task-3/finetuning-phi-2-text-summarization/reports/report_phi2_summarization.md) | Dataset Analysis, ROUGE Scores |

---

## ğŸ’¡ Key Learnings

### 1ï¸âƒ£ Single-Label vs Multi-Label Classification

```python
# Single-Label (AG News, MNLI) - Exactly ONE class per sample
loss_fn = CrossEntropyLoss()
prediction = torch.argmax(logits, dim=-1)

# Multi-Label (GoEmotions) - MULTIPLE classes possible
loss_fn = BCEWithLogitsLoss()
prediction = (torch.sigmoid(logits) > 0.5).int()
```

### 2ï¸âƒ£ Sentence Pair Encoding (NLI)

```python
# Single sentence (Classification)
tokenizer(text, max_length=128)
# â†’ [CLS] text [SEP]

# Sentence pair (NLI)
tokenizer(premise, hypothesis, max_length=256)
# â†’ [CLS] premise [SEP] hypothesis [SEP]
```

### 3ï¸âƒ£ Parameter-Efficient Fine-tuning (LoRA)

```python
# Instead of training 2.7B parameters...
# LoRA trains only 8.4M parameters (0.30%)!

lora_config = LoraConfig(
    r=16,           # Low-rank dimension
    lora_alpha=32,  # Scaling factor
    target_modules=["q_proj", "k_proj", "v_proj"],
    lora_dropout=0.05
)
```

---

## ğŸ“– References

1. Devlin, J., et al. (2019). **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**
2. Raffel, C., et al. (2020). **"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** (T5)
3. Microsoft Research (2023). **"Phi-2: The surprising power of small language models"**
4. Hu, E., et al. (2021). **"LoRA: Low-Rank Adaptation of Large Language Models"**
5. HuggingFace Transformers Documentation

---

## ğŸ“œ License

This project is created for **educational purposes** as part of the Deep Learning course final exam (UAS) at **Telkom University**.

---

## ğŸ™ Acknowledgments


| | |
|:-:|:-:|
| [ğŸ¤— HuggingFace](https://huggingface.co/) | Pre-trained models & datasets |
| [Google Colab](https://colab.research.google.com/) | Free GPU resources |
| [Telkom University](https://telkomuniversity.ac.id/) | Academic support |
| Course Instructors | Guidance & feedback |


---
