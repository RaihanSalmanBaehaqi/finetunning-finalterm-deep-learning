<div align="center">

# ğŸ¤– Task 2: Fine-tuning Encoder-Decoder Transformer

### T5 for Question Answering | UAS Deep Learning

[![T5](https://img.shields.io/badge/Model-T5--base-green?style=for-the-badge&logo=google&logoColor=white)](https://huggingface.co/google-t5/t5-base)
[![SQuAD](https://img.shields.io/badge/Dataset-SQuAD_v1.1-blue?style=for-the-badge)](https://rajpurkar.github.io/SQuAD-explorer/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.40+-FFD21E?style=for-the-badge)](https://huggingface.co/)

**Generative Question Answering dengan Text-to-Text Transfer Transformer**

[ğŸ“Š Results](#-results) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“ Structure](#-directory-structure)

---

### ğŸ¯ Performance Highlights

| Exact Match | F1 Score | Training Time | Parameters |
|:-----------:|:--------:|:-------------:|:----------:|
| **60.00%** | **77.59%** | ~20 min | 223M |

</div>

---

## ğŸ“‹ Overview

**Task 2** mengeksplorasi arsitektur **Encoder-Decoder Transformer** untuk task **Generative Question Answering**. Berbeda dengan Task 1 yang menggunakan encoder-only (BERT), task ini menggunakan **T5 (Text-to-Text Transfer Transformer)** yang dapat menghasilkan jawaban dalam bentuk teks.

### ğŸ“ Learning Objectives

| # | Objective | Status |
|:-:|:----------|:------:|
| 1 | Memahami arsitektur encoder-decoder Transformer | âœ… |
| 2 | Implementasi fine-tuning T5 untuk Question Answering | âœ… |
| 3 | Menerapkan text-to-text paradigm | âœ… |
| 4 | Evaluasi dengan SQuAD metrics (EM & F1) | âœ… |
| 5 | Optimasi untuk resource constraints | âœ… |

### ğŸ”„ Task 2 vs Task 1

| Aspect | Task 1 (BERT) | Task 2 (T5) |
|:------:|:-------------:|:-----------:|
| **Architecture** | Encoder-only | **Encoder-Decoder** |
| **Approach** | Extractive (span) | **Generative (text)** |
| **Output** | Start/End positions | **Generated sequence** |
| **Paradigm** | Task-specific head | **Text-to-Text** |
| **Flexibility** | Must extract from context | **Can generate freely** |

---

## ğŸ—ï¸ Architecture

### T5: Text-to-Text Transfer Transformer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ENCODER-DECODER ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   Input: "question: What is X? context: ... X is Y ..."            â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚     ENCODER     â”‚  â† Bidirectional            â”‚
â”‚                    â”‚   (12 layers)   â”‚     Understand context      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                             â”‚                                       â”‚
â”‚                      Hidden States                                  â”‚
â”‚                             â”‚                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚     DECODER     â”‚  â† Autoregressive           â”‚
â”‚                    â”‚   (12 layers)   â”‚     Generate answer         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                             â”‚                                       â”‚
â”‚                             â–¼                                       â”‚
â”‚                    Output: "Y" (Answer)                            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Property | Value |
|:---------|:------|
| **Model** | `google-t5/t5-base` |
| **Parameters** | 222,903,552 (~223M) |
| **Encoder Layers** | 12 |
| **Decoder Layers** | 12 |
| **Hidden Size** | 768 |
| **Attention Heads** | 12 |
| **Vocabulary** | 32,128 (SentencePiece) |

---

## ğŸ“Š Results

### Training Performance

| Epoch | Train Loss | Val Loss | Status |
|:-----:|:----------:|:--------:|:------:|
| 1 | 0.6357 | 0.0845 | âœ… |
| 2 | 0.0443 | 0.0862 | âœ… |

### Evaluation Metrics

| Metric | Score | Description |
|:------:|:-----:|:------------|
| **Exact Match** | **60.00%** | Perfect string match |
| **F1 Score** | **77.59%** | Token-level overlap |

### Prediction Quality

| Category | Percentage | Description |
|:--------:|:----------:|:------------|
| âœ… Perfect | **70%** | Exactly matches ground truth |
| ğŸŸ¢ Good | **10%** | F1 â‰¥ 0.7 |
| ğŸŸ¡ Partial | **5%** | 0.3 < F1 < 0.7 |
| âŒ Poor | **15%** | F1 â‰¤ 0.3 |

### Sample Predictions

| Question | Ground Truth | Prediction | Match |
|:---------|:-------------|:-----------|:-----:|
| "In what year did Massachusetts first require children to be educated?" | 1852 | 1852 | âœ… |
| "Why was this organization created?" | coordinate the response | to coordinate the response | ğŸŸ¢ |

---

## ğŸ“ Directory Structure

```
task-2/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              â† You are here!
â”‚
â””â”€â”€ ğŸ“‚ finetuning-t5-question-answering/      â† Main project
    â”‚
    â”œâ”€â”€ ğŸ“„ README.md                          # Detailed documentation
    â”œâ”€â”€ ğŸ“„ requirements.txt                   # Dependencies
    â”‚
    â”œâ”€â”€ ğŸ““ notebooks/
    â”‚   â””â”€â”€ finetuning_t5_question_answering.ipynb  # Main notebook
    â”‚
    â””â”€â”€ ğŸ“Š reports/
        â”œâ”€â”€ ğŸ“„ report_t5_qa.md                # Detailed report
        â”œâ”€â”€ ğŸ–¼ï¸ dataset_analysis.png           # Dataset visualization
        â”œâ”€â”€ ğŸ–¼ï¸ Training & Validation Loss.png # Loss curves
        â”œâ”€â”€ ğŸ–¼ï¸ Training_Config.png            # Configuration
        â”œâ”€â”€ ğŸ–¼ï¸ evaluation_metrics.png         # EM & F1 metrics
        â”œâ”€â”€ ğŸ–¼ï¸ Final_Results.png              # Summary
        â”œâ”€â”€ ğŸ–¼ï¸ F1_Distributions.png           # F1 histogram
        â”œâ”€â”€ ğŸ–¼ï¸ Model_Comparison.png           # Benchmarks
        â””â”€â”€ ğŸ–¼ï¸ Inferences_example.png         # Predictions
```

---

## ğŸ“š Dataset: SQuAD v1.1

**Stanford Question Answering Dataset** - Dataset benchmark untuk extractive QA.

| Split | Original | Used | Percentage |
|:-----:|:--------:|:----:|:----------:|
| Train | 87,599 | 4,379 | 5% |
| Validation | 10,570 | 1,057 | 10% |

### Input-Output Format

```python
# T5 Input Format
input = "question: What is the capital of France? context: Paris is the capital of France..."

# T5 Output
output = "Paris"
```

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|:----------|:------|
| Batch Size | 4 |
| Epochs | 2 |
| Learning Rate | 3e-4 |
| Optimizer | AdamW |
| Max Input Length | 256 |
| Max Output Length | 32 |
| Mixed Precision | FP16 |
| Warmup Steps | 200 |

---

## ğŸš€ Quick Start

### Google Colab (Recommended)

1. **Navigate to notebook:**
   ```
   task-2/finetuning-t5-question-answering/notebooks/
   ```

2. **Open in Colab:**
   - Upload `finetuning_t5_question_answering.ipynb`
   - Enable GPU: `Runtime â†’ Change runtime type â†’ GPU`

3. **Run all cells:**
   ```
   Runtime â†’ Run all (Ctrl+F9)
   ```

### Local Setup

```bash
# Navigate to project
cd task-2/finetuning-t5-question-answering

# Install dependencies
pip install -r requirements.txt

# Run Jupyter
jupyter notebook notebooks/finetuning_t5_question_answering.ipynb
```

---

## ğŸ‘¥ Team Information

| Name | NIM | Class |
|:-----|:---:|:-----:|
| Raihan Salman Baehaqi | 1103220180 | TK-46-02 |
| Jaka Kelana Wijaya | 1103223048 | TK-46-02 |

---

## ğŸ“– Documentation

| Document | Description | Link |
|:---------|:------------|:----:|
| **Project README** | Detailed documentation | [ğŸ“„](finetuning-t5-question-answering/README.md) |
| **Experiment Report** | Full analysis & results | [ğŸ“Š](finetuning-t5-question-answering/reports/report_t5_qa.md) |
| **Training Notebook** | Complete implementation | [ğŸ““](finetuning-t5-question-answering/notebooks/finetuning_t5_question_answering.ipynb) |

---

## ğŸ”— Related Tasks

| Task | Model | Architecture | Dataset | Status |
|:----:|:-----:|:------------:|:-------:|:------:|
| [Task 1](../task-1/) | BERT | Encoder | AG News, GoEmotions, MNLI | âœ… |
| **Task 2** | **T5** | **Encoder-Decoder** | **SQuAD** | âœ… |
| [Task 3](../task-3/) | Phi-2 | Decoder | XSum | âœ… |

---

## ğŸ“š Key Concepts

### Text-to-Text Paradigm

T5 menggunakan format unified untuk semua task:

```python
# Question Answering
"question: {Q} context: {C}" â†’ "{Answer}"

# Translation
"translate English to French: {text}" â†’ "{translation}"

# Summarization
"summarize: {document}" â†’ "{summary}"
```

### Encoder-Decoder vs Encoder-Only

| Aspect | Encoder-Only (BERT) | Encoder-Decoder (T5) |
|:-------|:--------------------|:---------------------|
| **Processing** | Bidirectional | Bidirectional â†’ Autoregressive |
| **Output** | Fixed-size logits | Variable-length sequence |
| **Training** | MLM + Classification | Denoising + Generation |
| **Use Case** | Understanding | Generation |

---

## ğŸ“œ License

Educational project for Deep Learning course (UAS) at Telkom University.

---

<div align="center">

**Part of UAS Deep Learning**

*Exploring Encoder-Decoder Transformer Architecture*

[![Task](https://img.shields.io/badge/Task-2_of_3-blue?style=for-the-badge)]()
[![Status](https://img.shields.io/badge/Status-âœ…_Completed-success?style=for-the-badge)]()

</div>
