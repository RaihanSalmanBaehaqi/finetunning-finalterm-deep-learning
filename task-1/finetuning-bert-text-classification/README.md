# ğŸ¤— Fine-tuning BERT for Text Classification

## ğŸ“‹ Overview

Repository ini berisi implementasi **fine-tuning model BERT** untuk task **Text Classification** sebagai bagian dari UAS Deep Learning. Project ini mengeksplorasi arsitektur **Transformer Encoder** untuk menyelesaikan dua jenis klasifikasi teks:

| Task | Dataset | Type | Classes | Best Metric |
|------|---------|------|---------|-------------|
| ğŸ“° News Classification | AG News | Multi-class | 4 | **94.75% Accuracy** |
| ğŸ˜Š Emotion Detection | GoEmotions | Multi-label | 28 | **57.49% Micro-F1** |

> **Note:** Task NLI (MNLI) disubmit di repository terpisah: [`finetuning-bert-nli`](https://github.com/[username]/finetuning-bert-nli)

---

## ğŸ‘¤ Identitas Tim

* RAIHAN SALMAN BAEHAQI (1103220180)
* JAKA KELANA WIJAYA (1103223048)

---

## ğŸ¯ Highlights

| Achievement | Value |
|-------------|-------|
| ğŸ† AG News Accuracy | **94.75%** (exceeds BERT paper benchmark!) |
| ğŸ† GoEmotions Micro-F1 | **57.49%** (matches benchmark) |
| âš¡ AG News Training Time | ~35 minutes |
| âš¡ GoEmotions Training Time | ~10.5 minutes |
| ğŸ”§ Total Parameters | ~109M (BERT-base) |

---

## ğŸ“š Datasets

### 1. ğŸ“° AG News (Multi-class Classification)

| Property | Value |
|----------|-------|
| **Source** | `sh0416/ag_news` (HuggingFace) |
| **Task** | 4-class news topic classification |
| **Train Samples** | 108,000 |
| **Validation Samples** | 12,000 |
| **Test Samples** | 7,600 |
| **Class Balance** | âœ… Perfectly balanced (25% each) |

**Labels:**

| ID | Category | Description |
|----|----------|-------------|
| 0 | ğŸŒ World | International news & politics |
| 1 | âš½ Sports | Sports news |
| 2 | ğŸ’¼ Business | Business & economics |
| 3 | ğŸ”¬ Sci/Tech | Science & technology |

### 2. ğŸ˜Š GoEmotions (Multi-label Classification)

| Property | Value |
|----------|-------|
| **Source** | `google-research-datasets/go_emotions` (HuggingFace) |
| **Config** | simplified (28 emotions) |
| **Task** | Multi-label emotion detection |
| **Train Samples** | ~43,000 |
| **Validation Samples** | ~5,400 |
| **Test Samples** | ~5,400 |
| **Class Balance** | âš ï¸ Severely imbalanced (300:1 ratio) |

**Labels (28 Emotions):**

| Category | Emotions |
|----------|----------|
| ğŸŸ¢ **Positive** | admiration, amusement, approval, caring, excitement, gratitude, joy, love, optimism, pride, relief |
| ğŸ”´ **Negative** | anger, annoyance, disappointment, disapproval, disgust, embarrassment, fear, grief, nervousness, remorse, sadness |
| ğŸŸ¡ **Ambiguous** | confusion, curiosity, desire, realization, surprise |
| âšª **Neutral** | neutral |

---

## ğŸ—‚ï¸ Model Architecture

| Property | Value |
|----------|-------|
| **Base Model** | `bert-base-uncased` |
| **Architecture** | Encoder-only (Bidirectional) |
| **Layers** | 12 |
| **Hidden Size** | 768 |
| **Attention Heads** | 12 |
| **Total Parameters** | ~109M |
| **Framework** | HuggingFace Transformers |

---

## âš™ï¸ Training Configuration

| Parameter | AG News | GoEmotions |
|-----------|---------|------------|
| Max Length | 128 | 128 |
| Epochs | 3 | 3 |
| Learning Rate | 2e-5 | 2e-5 |
| Batch Size (Train) | 16 | 16 |
| Batch Size (Eval) | 32 | 32 |
| Weight Decay | 0.01 | 0.01 |
| Warmup Ratio | 0.1 | 0.1 |
| Optimizer | AdamW | AdamW |
| FP16 | âœ… Enabled | âœ… Enabled |
| Loss Function | CrossEntropyLoss | BCEWithLogitsLoss |
| Seed | 42 | 42 |

---

## ğŸ“Š Results

### ğŸ“° AG News (Multi-class Classification)

| Split | Loss | Accuracy | Macro-F1 |
|-------|------|----------|----------|
| **Validation** | 0.1790 | **94.83%** | **94.81%** |
| **Test** | 0.1832 | **94.75%** | **94.76%** |

**Per-Class Performance (Test Set):**

| Class | Precision | Recall | F1-Score | Status |
|-------|-----------|--------|----------|--------|
| ğŸŒ World | 96.79% | 95.11% | 95.94% | ğŸŸ¢ Excellent |
| âš½ Sports | 98.64% | 99.11% | **98.87%** | ğŸŸ¢ Best |
| ğŸ’¼ Business | 91.09% | 92.58% | 91.83% | ğŸŸ¡ Good |
| ğŸ”¬ Sci/Tech | 92.55% | 92.21% | 92.38% | ğŸŸ¡ Good |

**Training Statistics:**
- â±ï¸ Training Time: **34.96 minutes**
- ğŸ“‰ Final Training Loss: **0.1747**

### ğŸ˜Š GoEmotions (Multi-label Classification)

| Split | Loss | Micro-F1 | Macro-F1 |
|-------|------|----------|----------|
| **Validation** | 0.0857 | **57.43%** | **39.91%** |
| **Test** | 0.0847 | **57.49%** | **39.50%** |

**Per-Class Performance Tiers:**

| Tier | F1 Range | Emotions |
|------|----------|----------|
| ğŸŸ¢ **Excellent** | 70-92% | gratitude (91.5%), amusement (81.0%), love (80.9%), admiration (71.1%) |
| ğŸŸ¡ **Good** | 50-70% | neutral, fear, joy, remorse, optimism, sadness, surprise |
| ğŸŸ  **Moderate** | 30-50% | anger, curiosity, desire, disgust, approval, caring, confusion |
| ğŸ”´ **Poor** | 0-30% | annoyance, disappointment, realization |
| âš« **Zero** | 0% | embarrassment, grief, nervousness, pride, relief âš ï¸ |

**Training Statistics:**
- â±ï¸ Training Time: **10.46 minutes**
- ğŸ“‰ Final Training Loss: **0.1115**

---

## ğŸ† Comparison with Benchmarks

### AG News

| Model | Accuracy | Source |
|-------|----------|--------|
| **BERT-base (Ours)** | **94.75%** âœ… | This repository |
| BERT-base (Paper) | 94.2% | Devlin et al. 2019 |
| DistilBERT | 93.8% | Sanh et al. 2019 |
| RoBERTa-base | 95.0% | Liu et al. 2019 |

> ğŸ‰ **Our implementation exceeds the original BERT paper benchmark!**

### GoEmotions

| Model | Micro-F1 | Macro-F1 | Source |
|-------|----------|----------|--------|
| **BERT-base (Ours)** | **57.49%** | **39.50%** | This repository |
| BERT-base (Paper) | 58.0% | 46.0% | Demszky et al. 2020 |
| RoBERTa-base | 59.1% | 48.2% | Demszky et al. 2020 |

> âœ… **Micro-F1 matches benchmark!** Macro-F1 lower due to 5 rare emotions with F1=0%.

---

## ğŸ“ Repository Structure

```
finetuning-bert-text-classification/
â”œâ”€â”€ ğŸ“„ README.md                              # Project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt                       # Python dependencies
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”œâ”€â”€ finetune_bert_ag_news.ipynb          # AG News training (24 sections)
â”‚   â””â”€â”€ finetune_bert_go_emotions.ipynb      # GoEmotions training (20 sections)
â”œâ”€â”€ ğŸ“Š reports/
â”‚   â”œâ”€â”€ report_ag_news.md                    # AG News detailed report
â”‚   â””â”€â”€ report_go_emotions.md                # GoEmotions detailed report
â””â”€â”€ ğŸ¤– models/
    â””â”€â”€ (saved model checkpoints)            # Best models saved here
```

---

## ğŸš€ How to Run

### Option 1: Google Colab (Recommended) â­

1. **Upload notebook** ke Google Colab
2. **Enable GPU runtime:**
   ```
   Runtime â†’ Change runtime type â†’ GPU (T4)
   ```
3. **Mount Google Drive:**
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```
4. **Run all cells:**
   ```
   Runtime â†’ Run all (Ctrl+F9)
   ```

### Option 2: Local Environment

```bash
# 1. Clone repository
git clone https://github.com/[username]/finetuning-bert-text-classification.git
cd finetuning-bert-text-classification

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run Jupyter
jupyter notebook notebooks/
```

---

## ğŸ““ Notebooks Overview

### ğŸ“° `finetune_bert_ag_news.ipynb` (24 Sections)

| Section | Content |
|---------|---------|
| 0-1 | Mount Drive & Setup |
| 2-3 | Install Dependencies & Imports |
| 4 | Configuration |
| 5-8 | Load Dataset & EDA |
| 9-12 | Preprocessing & Tokenization |
| 13-16 | Model Setup & Sanity Checks |
| 17-18 | Training |
| 19-21 | Evaluation & Analysis |
| 22-24 | Save Model & Inference Demo |

### ğŸ˜Š `finetune_bert_go_emotions.ipynb` (20 Sections)

| Section | Content |
|---------|---------|
| 0-1 | Mount Drive & Setup |
| 2-3 | Install Dependencies & Imports |
| 4 | Configuration |
| 5-7 | Load Dataset & EDA |
| 8-10 | Multi-hot Encoding & Tokenization |
| 11-13 | Model Setup (multi_label_classification) |
| 14-15 | Training |
| 16-18 | Evaluation & Per-Class Analysis |
| 19-20 | Save Model & Inference Demo |

---

## ğŸ”‘ Key Implementation Differences

| Aspect | AG News (Single-Label) | GoEmotions (Multi-Label) |
|--------|------------------------|--------------------------|
| **Labels per sample** | Exactly 1 | 0 to many |
| **Label encoding** | Integer (0-3) | Multi-hot float32 (28-dim) |
| **Loss function** | CrossEntropyLoss | BCEWithLogitsLoss |
| **Activation** | Softmax | Sigmoid |
| **Prediction** | `argmax(logits)` | `sigmoid(logits) > 0.5` |
| **Primary metric** | Accuracy | Micro-F1 |
| **problem_type** | `single_label_classification` | `multi_label_classification` |

### âš ï¸ Critical for Multi-Label (GoEmotions)

```python
# 1. Model must use multi_label_classification
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=28,
    problem_type="multi_label_classification"  # CRITICAL!
)

# 2. Labels must be float32 (not int)
labels = np.zeros(28, dtype=np.float32)

# 3. Prediction uses sigmoid + threshold
probs = torch.sigmoid(logits)
predictions = (probs > 0.5).int()
```

---

## ğŸ“ˆ Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ğŸ“¥ Load Dataset (HuggingFace Datasets)                  â”‚
â”‚         â†“                                                    â”‚
â”‚  2. ğŸ” Exploratory Data Analysis (EDA)                      â”‚
â”‚         â†“                                                    â”‚
â”‚  3. âœ‚ï¸  Train/Validation Split                              â”‚
â”‚         â†“                                                    â”‚
â”‚  4. ğŸ”¤ Tokenization (AutoTokenizer)                         â”‚
â”‚         â†“                                                    â”‚
â”‚  5. ğŸ¤– Load Pre-trained BERT                                â”‚
â”‚         â†“                                                    â”‚
â”‚  6. âš™ï¸  Configure TrainingArguments                         â”‚
â”‚         â†“                                                    â”‚
â”‚  7. ğŸ‹ï¸ Fine-tuning with Trainer API                         â”‚
â”‚         â†“                                                    â”‚
â”‚  8. ğŸ“Š Evaluation (Metrics + Confusion Matrix)              â”‚
â”‚         â†“                                                    â”‚
â”‚  9. ğŸ’¾ Save Best Model                                      â”‚
â”‚         â†“                                                    â”‚
â”‚  10. ğŸ¯ Inference Demo                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technologies Used

| Technology | Purpose |
|------------|---------|
| **PyTorch** | Deep learning framework |
| **Transformers** | Pre-trained BERT models |
| **Datasets** | HuggingFace datasets |
| **Evaluate** | Metrics computation |
| **Accelerate** | Training optimization |
| **scikit-learn** | Classification report & confusion matrix |
| **Matplotlib/Seaborn** | Visualization |

---

## ğŸ“ Reports

Detailed experiment reports available in `reports/` folder:

| Report | Description | Link |
|--------|-------------|------|
| ğŸ“° AG News | Multi-class classification results | [report_ag_news.md](reports/report_ag_news.md) |
| ğŸ˜Š GoEmotions | Multi-label classification results | [report_go_emotions.md](reports/report_go_emotions.md) |

---

## ğŸ’¡ Lessons Learned

### AG News (Single-Label)
1. âœ… BERT excels at news classification (94.75% accuracy)
2. âœ… Sports is easiest to classify (distinctive vocabulary)
3. âš ï¸ Business â†” Sci/Tech sometimes confused (tech company news)

### GoEmotions (Multi-Label)
1. âœ… Multi-label is significantly harder than single-label
2. âš ï¸ Class imbalance is critical (5 emotions with F1=0%)
3. ğŸ’¡ Per-class threshold tuning could improve Macro-F1
4. ğŸ’¡ Rare emotions need special handling (weighted loss, data augmentation)

---

## ğŸ”— Related Repositories

| Repository | Task | Model | Dataset |
|------------|------|-------|---------|
| **This repo** | Text Classification | BERT | AG News, GoEmotions |
| [`finetuning-bert-nli`](https://github.com/[username]/finetuning-bert-nli) | NLI | BERT | MNLI |
| [`finetuning-t5-question-answering`](https://github.com/[username]/finetuning-t5-question-answering) | Question Answering | T5 | SQuAD |
| [`finetuning-phi2-text-summarization`](https://github.com/[username]/finetuning-phi2-text-summarization) | Summarization | Phi-2 | CNN/DailyMail |

---

## ğŸ“œ License

This project is created for **educational purposes** as part of Deep Learning course final exam (UAS).

---

## ğŸ™ Acknowledgments

- [HuggingFace](https://huggingface.co/) for pre-trained models and datasets
- [Google Colab](https://colab.research.google.com/) for free GPU resources
- Course instructors for guidance and support
- Original paper authors: Devlin et al. (BERT), Demszky et al. (GoEmotions)

---

## ğŸ“š References

1. Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
2. Zhang, X., et al. (2015). "Character-level Convolutional Networks for Text Classification" (AG News)
3. Demszky, D., et al. (2020). "GoEmotions: A Dataset of Fine-Grained Emotions"
4. HuggingFace Transformers Documentation

---
