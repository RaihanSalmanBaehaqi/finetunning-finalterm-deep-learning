# ğŸ¤— Fine-tuning BERT for Text Classification

## ğŸ“‹ Overview

Repository ini berisi implementasi **fine-tuning model BERT** untuk task **Text Classification** sebagai bagian dari UAS Deep Learning. Project ini mengeksplorasi arsitektur **Transformer Encoder** untuk menyelesaikan dua jenis klasifikasi teks:

1. **Multi-class Classification** (AG News) - Klasifikasi berita ke 4 kategori
2. **Multi-label Classification** (GoEmotions) - Deteksi multi-emosi dalam teks

> **Note:** Task NLI (MNLI) disubmit di repository terpisah: `finetuning-bert-nli`

---

## ğŸ‘¤ Identitas

| Field | Value |
|-------|-------|
| **Nama** | [Nama Lengkap Anda] |
| **NIM** | [NIM Anda] |
| **Kelas** | TK-46-02 |
| **Mata Kuliah** | Deep Learning |

---

## ğŸ“š Datasets

### 1. AG News (Multi-class)
- **Source:** `sh0416/ag_news` (HuggingFace)
- **Task:** 4-class news topic classification
- **Labels:** World (0), Sports (1), Business (2), Sci/Tech (3)
- **Size:** 120,000 train / 7,600 test

### 2. GoEmotions (Multi-label)
- **Source:** `google-research-datasets/go_emotions` (HuggingFace)
- **Task:** Multi-label emotion detection
- **Labels:** 28 emotion categories
- **Size:** ~58,000 samples

---

## ğŸ—ï¸ Model Architecture

- **Base Model:** `bert-base-uncased`
- **Framework:** HuggingFace Transformers
- **Architecture Type:** Encoder-only (Bidirectional)

---

## âš™ï¸ Training Configuration

| Parameter | AG News | GoEmotions |
|-----------|---------|------------|
| Max Length | 128 | 128 |
| Epochs | 3 | 3 |
| Learning Rate | 2e-5 | 2e-5 |
| Batch Size (Train) | 16 | 16 |
| Batch Size (Eval) | 32 | 32 |
| Weight Decay | 0.01 | 0.01 |
| Optimizer | AdamW | AdamW |
| FP16 | âœ… (if CUDA) | âœ… (if CUDA) |

---

## ğŸ“Š Results

### AG News (Multi-class Classification)

| Split | Loss | Accuracy | Macro-F1 |
|-------|------|----------|----------|
| Validation | - | ~94% | ~94% |
| Test | - | ~94% | ~94% |

### GoEmotions (Multi-label Classification)

| Split | Loss | Micro-F1 | Macro-F1 |
|-------|------|----------|----------|
| Validation | - | ~50% | ~40% |
| Test | - | ~50% | ~40% |

> **Note:** Hasil akan diupdate setelah training selesai.

---

## ğŸ“ Repository Structure

```
finetuning-bert-text-classification/
â”œâ”€â”€ README.md                              # Project documentation
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ finetune_bert_ag_news.ipynb       # AG News training notebook
â”‚   â””â”€â”€ finetune_bert_go_emotions.ipynb   # GoEmotions training notebook
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ report_ag_news.md                 # AG News experiment report
â”‚   â””â”€â”€ report_go_emotions.md             # GoEmotions experiment report
â””â”€â”€ models/
    â””â”€â”€ (saved model files)               # Best model checkpoints
```

---

## ğŸš€ How to Run

### 1. Clone Repository
```bash
git clone https://github.com/[username]/finetuning-bert-text-classification.git
cd finetuning-bert-text-classification
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run on Google Colab (Recommended)

1. Upload notebook ke Google Colab
2. Mount Google Drive:
```python
from google.colab import drive
drive.mount('/content/drive')
```

3. Set project directory:
```python
PROJECT_DIR = "/content/drive/MyDrive/finetuning-bert-text-classification"
```

4. Run all cells

---

## ğŸ““ Notebooks

| Notebook | Description | Dataset |
|----------|-------------|---------|
| `finetune_bert_ag_news.ipynb` | Multi-class text classification | AG News |
| `finetune_bert_go_emotions.ipynb` | Multi-label emotion detection | GoEmotions |

---

## ğŸ” Key Implementation Details

### AG News (Single-Label)
- Uses `CrossEntropyLoss` (automatic)
- Prediction: `argmax` of logits
- Metrics: Accuracy, Macro-F1

### GoEmotions (Multi-Label)
- Uses `BCEWithLogitsLoss`
- Prediction: `sigmoid > 0.5` threshold
- Metrics: Micro-F1, Macro-F1
- Requires one-hot encoding for labels

---

## ğŸ“ˆ Training Pipeline

```
1. Load Dataset (HuggingFace Datasets)
         â†“
2. Preprocessing & Tokenization
         â†“
3. Train/Validation Split
         â†“
4. Load Pre-trained BERT
         â†“
5. Fine-tuning with Trainer API
         â†“
6. Evaluation (Metrics + Confusion Matrix)
         â†“
7. Save Best Model
         â†“
8. Inference Demo
```

---

## ğŸ› ï¸ Technologies Used

- **PyTorch** - Deep learning framework
- **Transformers** - HuggingFace transformers library
- **Datasets** - HuggingFace datasets library
- **Evaluate** - HuggingFace evaluation metrics
- **scikit-learn** - ML utilities & metrics
- **Accelerate** - Training optimization

---

## ğŸ“ Reports

Detailed experiment reports are available in the `reports/` folder:
- [AG News Report](reports/report_ag_news.md)
- [GoEmotions Report](reports/report_go_emotions.md)

---

## ğŸ”— Related Repositories

- **Task 1 (NLI):** `finetuning-bert-nli` - BERT for Natural Language Inference
- **Task 2:** `finetuning-t5-question-answering` - T5 for Question Answering
- **Task 3:** `finetuning-phi2-text-summarization` - Phi-2 for Text Summarization

---

## ğŸ“œ License

This project is created for educational purposes as part of Deep Learning course final exam (UAS).

---

## ğŸ™ Acknowledgments

- HuggingFace for providing pre-trained models and datasets
- Google Colab for free GPU resources
- Course instructors for guidance
