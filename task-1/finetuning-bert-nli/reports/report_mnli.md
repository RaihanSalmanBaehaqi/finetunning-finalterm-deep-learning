# üìä Task 1 Report: Fine-tuning BERT for MNLI (Natural Language Inference)

## 1. Overview

Laporan ini mendokumentasikan implementasi **fine-tuning BERT** untuk task **Natural Language Inference (NLI)** menggunakan dataset **MNLI**. Task NLI bertujuan untuk menentukan hubungan logika antara dua kalimat: **premise** dan **hypothesis**.

| Item | Detail |
|------|--------|
| **Model** | `bert-base-uncased` |
| **Dataset** | MNLI (Multi-Genre NLI) via GLUE |
| **Task Type** | 3-class Classification |
| **Labels** | Entailment, Neutral, Contradiction |
| **Metrics** | Accuracy, Macro-F1 |

---

## 2. Dataset Description

### MNLI (Multi-Genre Natural Language Inference)

MNLI adalah dataset benchmark untuk NLI yang berisi pasangan kalimat dari berbagai genre teks, dikembangkan oleh NYU sebagai bagian dari GLUE benchmark.

### Label Categories

| Label | ID | Description | Semantic Meaning |
|-------|----|-----------| -----------------|
| üü¢ **Entailment** | 0 | Hypothesis logically follows from premise | If premise is true, hypothesis MUST be true |
| üü° **Neutral** | 1 | Hypothesis might be true, but not guaranteed | Premise doesn't give enough info |
| üî¥ **Contradiction** | 2 | Hypothesis contradicts premise | If premise is true, hypothesis MUST be false |

### Examples

| Premise | Hypothesis | Label | Explanation |
|---------|------------|-------|-------------|
| "A man is playing guitar" | "Someone is making music" | üü¢ Entailment | Guitar ‚Üí music (logical inference) |
| "A man is playing guitar" | "The man is a professional" | üü° Neutral | Could be amateur or professional |
| "A man is playing guitar" | "Nobody is playing any instrument" | üî¥ Contradiction | Direct negation |

### Data Splits

| Split | Samples | Description |
|-------|---------|-------------|
| Train | 392,702 | Training data from multiple genres |
| Validation (Matched) | 9,815 | Same genres as training |
| Validation (Mismatched) | 9,832 | Different genres from training |
| Test (Matched) | 9,796 | Hidden labels (GLUE leaderboard) |
| Test (Mismatched) | 9,847 | Hidden labels (GLUE leaderboard) |

**Genres in MNLI:**
- Fiction, Government, Telephone, Travel, Letters (Matched)
- 9/11, Face-to-face, Fiction, Government, Letters, OUP, Slate, Telephone, Travel, Verbatim (Mismatched)

### Class Distribution (Training Set)

| Class | Samples | Percentage |
|-------|---------|------------|
| Entailment | ~130,900 | 33.3% |
| Neutral | ~130,900 | 33.3% |
| Contradiction | ~130,900 | 33.3% |

> ‚úÖ Dataset ini **perfectly balanced** - setiap kelas memiliki jumlah sampel yang sama.

---

## 3. Methodology

### 3.1 Input Format

NLI berbeda dari text classification karena membutuhkan **2 kalimat** sebagai input:

```
Input: [CLS] premise [SEP] hypothesis [SEP]
```

**Token Type IDs:**
- Premise tokens: 0
- Hypothesis tokens: 1

### 3.2 Tokenization

```python
# Tokenize sentence pair
tokenizer(
    premise,           # First sentence
    hypothesis,        # Second sentence
    truncation=True,
    max_length=256     # Longer because 2 sentences
)
```

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Tokenizer | `bert-base-uncased` | Match pretrained model |
| Max Length | 256 | Accommodate 2 sentences |
| Truncation | True | Handle long inputs |
| Padding | Dynamic | Efficient batching |

### 3.3 Model Configuration

```python
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3,  # entailment, neutral, contradiction
    id2label=id2label,
    label2id=label2id,
)
```

**Model Architecture:**
- **Base:** BERT-base-uncased (12 layers, 768 hidden, 12 heads)
- **Classification Head:** Linear layer (768 ‚Üí 3)
- **Total Parameters:** ~109M
- **Trainable Parameters:** ~109M (full fine-tuning)

### 3.4 Training Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Max Length | 256 | Accommodate premise + hypothesis |
| Epochs | 3 | Standard for BERT fine-tuning |
| Learning Rate | 2e-5 | Recommended for BERT |
| Train Batch Size | 16 | Balance between speed and memory |
| Eval Batch Size | 32 | Larger for faster evaluation |
| Weight Decay | 0.01 | Regularization |
| Warmup Ratio | 0.1 | 10% warmup steps |
| Optimizer | AdamW | Standard for transformers |
| FP16 | Enabled | Mixed precision for speed |
| Seed | 42 | Reproducibility |

---

## 4. Results

### 4.1 Training Progress

| Epoch | Training Loss | Val Loss (Matched) | Val Accuracy | Val F1 |
|-------|---------------|-------------------|--------------|--------|
| 1 | ~0.50 | ~0.58 | ~82% | ~82% |
| 2 | ~0.40 | ~0.55 | ~84% | ~84% |
| 3 | ~0.38 | ~0.55 | ~84.7% | ~84.6% |

**Training Statistics:**

| Metric | Value |
|--------|-------|
| Total Training Time | **113.06 minutes** (6,784 sec) |
| Final Training Loss | **0.3825** |
| Training Samples | 392,702 |
| Best Epoch | 3 |

### 4.2 Evaluation Metrics

| Split | Loss | Accuracy | Macro-F1 |
|-------|------|----------|----------|
| **Validation Matched** | 0.5522 | **84.67%** | **84.63%** |
| **Validation Mismatched** | 0.5337 | **84.74%** | **84.69%** |

> ‚úÖ Matched ‚âà Mismatched menunjukkan **excellent generalization** across genres!

### 4.3 Per-Class Performance (Validation Matched)

| Class | Precision | Recall | F1-Score | Support | Status |
|-------|-----------|--------|----------|---------|--------|
| üü¢ Entailment | 89.69% | 84.28% | **86.90%** | 3,479 | ü•á Best |
| üî¥ Contradiction | 86.04% | 86.68% | **86.36%** | 3,213 | ü•à Great |
| üü° Neutral | 78.36% | 83.03% | **80.63%** | 3,123 | ü•â Hardest |

**Performance Ranking:**
1. ü•á **Entailment** (86.90% F1) - Clearest logical relationship
2. ü•à **Contradiction** (86.36% F1) - Direct negation is detectable
3. ü•â **Neutral** (80.63% F1) - Semantically ambiguous

---

## 5. Analysis

### 5.1 Confusion Matrix (Validation Matched)

|  | Entailment (Pred) | Neutral (Pred) | Contradiction (Pred) |
|--|-------------------|----------------|----------------------|
| **Entailment (Actual)** | **2,932** | 380 | 167 |
| **Neutral (Actual)** | 296 | **2,594** | 233 |
| **Contradiction (Actual)** | 41 | 337 | **2,835** |

**Key Observations:**
- ‚úÖ **Entailment well-detected** - High precision (89.69%)
- ‚úÖ **Contradiction reliable** - Balanced precision/recall
- ‚ö†Ô∏è **Neutral often confused** - 380 entailment + 337 contradiction misclassified as neutral

### 5.2 Error Analysis

**Total Errors:** 1,454 / 9,815 = **14.82%**

**Most Common Misclassifications:**

| True Label | Predicted As | Count | Percentage | Reason |
|------------|--------------|-------|------------|--------|
| Entailment | Neutral | 380 | 26.1% | Inference not obvious |
| Neutral | Contradiction | 233 | 16.0% | Subtle negation |
| Contradiction | Neutral | 337 | 23.2% | Negation not detected |
| Neutral | Entailment | 296 | 20.4% | Hypothesis seems plausible |

### 5.3 Why Neutral is the Hardest Class?

| Challenge | Explanation | Example |
|-----------|-------------|---------|
| **Semantically ambiguous** | Neither clearly follows nor contradicts | "Man plays guitar" ‚Üí "Man is talented" |
| **Requires world knowledge** | Need to know what's NOT stated | "Woman reads book" ‚Üí "Woman reads Stephen King" |
| **Subtle certainty** | Difference between "probably" and "definitely" | "It's raining" ‚Üí "People carry umbrellas" |
| **Between two extremes** | Boundary between entailment & contradiction | Hard to define precisely |

### 5.4 Matched vs Mismatched Analysis

| Metric | Matched | Mismatched | Difference |
|--------|---------|------------|------------|
| Accuracy | 84.67% | 84.74% | +0.07% |
| Macro-F1 | 84.63% | 84.69% | +0.06% |
| Loss | 0.5522 | 0.5337 | -0.0185 |

> ‚úÖ **Excellent generalization!** Model performs equally well on unseen genres.

### 5.5 Key Observations

1. ‚úÖ **Matches BERT benchmark** (84.67% vs paper's 84.6%)
2. ‚úÖ **Excellent generalization** across genres (matched ‚âà mismatched)
3. ‚úÖ **Entailment best performer** (86.90% F1) - clearest logical signal
4. ‚ö†Ô∏è **Neutral is hardest** (80.63% F1) - semantically ambiguous
5. ‚úÖ **No overfitting** - similar performance on both validation sets

---

## 6. Key Differences from Text Classification

| Aspect | AG News (Classification) | MNLI (NLI) |
|--------|--------------------------|------------|
| **Input** | Single text | Two sentences (premise + hypothesis) |
| **Tokenization** | `tokenizer(text)` | `tokenizer(premise, hypothesis)` |
| **Max Length** | 128 | 256 |
| **Classes** | 4 (topics) | 3 (logical relations) |
| **Task Type** | What is it about? | Does B follow from A? |
| **Best Accuracy** | 94.75% | 84.67% |
| **Difficulty** | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê‚≠ê Hard |
| **Reasoning** | Surface patterns | Semantic inference |
| **Training Time** | 35 min | 113 min |
| **Dataset Size** | 120K | 393K |

**Why NLI is Harder:**
1. **Semantic reasoning required** - not just pattern matching
2. **Two inputs** - must understand relationship between sentences
3. **Neutral class** - inherently ambiguous category
4. **World knowledge** - often needed for inference

---

## 7. Inference Examples

### Example 1: Entailment ‚úÖ
**Premise:** "A man is playing a guitar on stage."  
**Hypothesis:** "Someone is making music."  
**Prediction:** Entailment (92.3% confidence)  
**Reasoning:** Guitar playing ‚Üí making music (logical implication)

### Example 2: Neutral ‚úÖ
**Premise:** "A woman is reading a book."  
**Hypothesis:** "The woman is reading a novel by Stephen King."  
**Prediction:** Neutral (78.5% confidence)  
**Reasoning:** Could be any book, not necessarily Stephen King

### Example 3: Contradiction ‚úÖ
**Premise:** "The restaurant is empty."  
**Hypothesis:** "The restaurant is crowded with customers."  
**Prediction:** Contradiction (96.1% confidence)  
**Reasoning:** "Empty" directly contradicts "crowded"

### Example 4: Challenging Case
**Premise:** "It is raining heavily outside."  
**Hypothesis:** "The sun is shining brightly."  
**Prediction:** Contradiction (88.7% confidence)  
**Reasoning:** Heavy rain typically means no sunshine (but could technically coexist with sun shower)

### Example 5: Subtle Neutral
**Premise:** "The children are playing in the park."  
**Hypothesis:** "The children are having a birthday party."  
**Prediction:** Neutral (71.2% confidence)  
**Reasoning:** Playing doesn't imply birthday party specifically

---

## 8. Comparison with Benchmarks

| Model | MNLI-Matched | MNLI-Mismatched | Parameters | Source |
|-------|--------------|-----------------|------------|--------|
| **BERT-base (Ours)** | **84.67%** | **84.74%** | 109M | This report |
| BERT-base (Paper) | 84.6% | 83.4% | 109M | Devlin et al. 2019 |
| RoBERTa-base | 87.6% | 87.4% | 125M | Liu et al. 2019 |
| ALBERT-base | 84.6% | 84.2% | 12M | Lan et al. 2020 |
| DistilBERT | 82.2% | 81.5% | 66M | Sanh et al. 2019 |
| DeBERTa-v3-base | 90.5% | 90.2% | 184M | He et al. 2021 |
| XLNet-base | 86.8% | 85.8% | 110M | Yang et al. 2019 |

> üéâ **Our implementation matches/exceeds the original BERT paper!**
> - MNLI-Matched: 84.67% vs 84.6% (+0.07%)
> - MNLI-Mismatched: 84.74% vs 83.4% (+1.34%)

---

## 9. Technical Implementation

### 9.1 Sentence Pair Tokenization

```python
def tokenize_function(batch):
    """Tokenize sentence pairs (premise + hypothesis)"""
    return tokenizer(
        batch["premise"],
        batch["hypothesis"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False  # Dynamic padding
    )
```

### 9.2 Input Format Visualization

```
Premise:    "A man is playing guitar"
Hypothesis: "Someone is making music"

Tokenized:
[CLS] a man is playing guitar [SEP] someone is making music [SEP]
  0   0  0  0    0      0      0      1       1    1      1    1

Token Type IDs:
- 0 for premise tokens
- 1 for hypothesis tokens
```

### 9.3 Inference Function

```python
def predict_nli(premise, hypothesis, model, tokenizer, device):
    model.eval()
    
    # Tokenize sentence pair
    inputs = tokenizer(
        premise, hypothesis,
        return_tensors="pt",
        truncation=True,
        max_length=256
    ).to(device)
    
    # Forward pass
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1)
        pred = torch.argmax(probs, dim=-1)
    
    return {
        "prediction": id2label[pred.item()],
        "confidence": probs[0][pred].item(),
        "all_probs": {
            "entailment": probs[0][0].item(),
            "neutral": probs[0][1].item(),
            "contradiction": probs[0][2].item()
        }
    }
```

---

## 10. Saved Artifacts

| Artifact | Location | Size |
|----------|----------|------|
| Best Model | `models/bert_mnli_best/` | ~420MB |
| Tokenizer | `models/bert_mnli_best/` | ~1MB |
| Confusion Matrix | `reports/mnli_confusion_matrix.png` | ~150KB |
| Label Distribution | `reports/mnli_label_distribution.png` | ~100KB |
| Training Logs | `outputs/bert_mnli/logs/` | ~10MB |

---

## 11. Potential Improvements

| Improvement | Expected Impact | Difficulty | Priority |
|-------------|-----------------|------------|----------|
| Use RoBERTa-base | +3% accuracy | ‚≠ê Easy | üî¥ High |
| Use DeBERTa-v3-base | +5-6% accuracy | ‚≠ê Easy | üî¥ High |
| Increase epochs to 4 | +0.3-0.5% accuracy | ‚≠ê Easy | üü° Medium |
| Learning rate 3e-5 | +0.2-0.3% accuracy | ‚≠ê Easy | üü° Medium |
| Longer max_length (384) | +0.2% on long samples | ‚≠ê Easy | üü¢ Low |
| Data augmentation (paraphrase) | +1-2% accuracy | ‚≠ê‚≠ê Medium | üü° Medium |
| Ensemble (BERT + RoBERTa) | +1-2% accuracy | ‚≠ê‚≠ê Medium | üü¢ Low |
| Focal loss for neutral | +1% on neutral class | ‚≠ê‚≠ê Medium | üü° Medium |

---

## 12. Understanding NLI Task

### 12.1 What Makes NLI Unique?

| Property | Description |
|----------|-------------|
| **Sentence pair input** | Model must understand relationship between 2 texts |
| **Logical reasoning** | Not pattern matching, but semantic inference |
| **Asymmetric** | Order matters: P‚ÜíH ‚â† H‚ÜíP |
| **World knowledge** | Sometimes required for correct inference |

### 12.2 NLI vs Other Tasks

| Task | Question Answered |
|------|-------------------|
| Text Classification | "What topic is this about?" |
| Sentiment Analysis | "Is this positive or negative?" |
| **NLI** | "If A is true, what can we say about B?" |
| Question Answering | "What is the answer to this question?" |

### 12.3 Applications of NLI

1. **Fact verification** - Does evidence support a claim?
2. **Question answering** - Is this answer consistent with context?
3. **Summarization evaluation** - Does summary entail source?
4. **Dialogue systems** - Is response consistent with conversation?

---

## 13. Conclusion

### ‚úÖ Achievements

1. **Benchmark-level performance:** 84.67% accuracy (matches BERT paper's 84.6%)
2. **Excellent generalization:** Matched (84.67%) ‚âà Mismatched (84.74%)
3. **Best on entailment:** 86.90% F1 (clearest logical relationship)
4. **No overfitting:** Similar performance across validation sets
5. **Sentence pair encoding works:** Successfully captures premise-hypothesis relationship

### ‚ö†Ô∏è Limitations

1. **Neutral class hardest:** 80.63% F1 (semantically ambiguous)
2. **Long training time:** 113 minutes (large dataset: 393K samples)
3. **World knowledge gaps:** Some inferences require external knowledge
4. **Lexical overlap bias:** Model may rely on word overlap rather than semantics

### üéØ Key Takeaways

1. **NLI is harder than classification** - 84.67% vs 94.75% accuracy
2. **Neutral is inherently difficult** - All models struggle with this class
3. **Sentence pair encoding essential** - `tokenizer(premise, hypothesis)` format
4. **Generalization is excellent** - Model handles new genres well
5. **Max length 256 recommended** - Accommodate both sentences

---

## 14. References

1. Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
2. Williams, A., et al. (2018). "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference" (MNLI)
3. Wang, A., et al. (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
4. HuggingFace Transformers Documentation

---

## Appendix A: Hardware Specifications

| Component | Specification |
|-----------|---------------|
| GPU | Tesla T4 (16GB VRAM) |
| Platform | Google Colab |
| CUDA Version | 11.8 |
| PyTorch Version | 2.0+ |
| Transformers Version | 4.35+ |

---

## Appendix B: Full Classification Report (Validation Matched)

```
               precision    recall  f1-score   support

   entailment     0.8969    0.8428    0.8690      3479
      neutral     0.7836    0.8303    0.8063      3123
contradiction     0.8604    0.8668    0.8636      3213

     accuracy                         0.8467      9815
    macro avg     0.8470    0.8466    0.8463      9815
 weighted avg     0.8489    0.8467    0.8473      9815
```

---

## Appendix C: Genre Distribution in MNLI

**Matched Genres (same as training):**
| Genre | Description |
|-------|-------------|
| Fiction | Literary fiction texts |
| Government | Government reports and documents |
| Telephone | Telephone conversation transcripts |
| Travel | Travel guides |
| Letters | Personal and formal letters |

**Mismatched Genres (different from training):**
| Genre | Description |
|-------|-------------|
| 9/11 | 9/11 Commission Report |
| Face-to-face | Face-to-face conversation transcripts |
| Slate | Slate magazine articles |
| Verbatim | Verbatim transcripts |
| OUP | Oxford University Press texts |

---
